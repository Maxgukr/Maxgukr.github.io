<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>paper reading solve combination problem with NN and RL | Maxwei | My personal blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="强化学习解决TSP路径规划问题问题背景接触组合优化问题源于参加过的比赛的一些经历，例如华为软挑的云服务器资源分配问题，阿里巴巴调度大赛的虚拟机放置，都属于组合优化问题。传统解决组合优化问题的方法关键在于设计启发式函数，这种方法非常依赖于具体的问题背景，例如装箱/背包问题与旅行商邮路问题都是np-hard，但是面对的背景不一样。而且启发函数的好坏对结果影响很大，因而难以泛华。在大数据时代，可以考虑利">
<meta name="keywords" content="strive seek find">
<meta property="og:type" content="article">
<meta property="og:title" content="paper reading solve combination problem with NN and RL">
<meta property="og:url" content="http://yoursite.com/2019/01/20/2019-1-20-LearningToSolveRoutingProblem/index.html">
<meta property="og:site_name" content="Maxwei | My personal blog">
<meta property="og:description" content="强化学习解决TSP路径规划问题问题背景接触组合优化问题源于参加过的比赛的一些经历，例如华为软挑的云服务器资源分配问题，阿里巴巴调度大赛的虚拟机放置，都属于组合优化问题。传统解决组合优化问题的方法关键在于设计启发式函数，这种方法非常依赖于具体的问题背景，例如装箱/背包问题与旅行商邮路问题都是np-hard，但是面对的背景不一样。而且启发函数的好坏对结果影响很大，因而难以泛华。在大数据时代，可以考虑利">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2019/01/20/2019-1-20-LearningToSolveRoutingProblem/img/MHA-FF.png">
<meta property="og:image" content="http://yoursite.com/2019/01/20/2019-1-20-LearningToSolveRoutingProblem/img/MHA.png">
<meta property="og:image" content="http://yoursite.com/2019/01/20/2019-1-20-LearningToSolveRoutingProblem/img/decode.jpg">
<meta property="og:image" content="http://yoursite.com/2019/01/20/2019-1-20-LearningToSolveRoutingProblem/img/REINFORCE.png">
<meta property="og:updated_time" content="2019-05-03T14:55:43.309Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="paper reading solve combination problem with NN and RL">
<meta name="twitter:description" content="强化学习解决TSP路径规划问题问题背景接触组合优化问题源于参加过的比赛的一些经历，例如华为软挑的云服务器资源分配问题，阿里巴巴调度大赛的虚拟机放置，都属于组合优化问题。传统解决组合优化问题的方法关键在于设计启发式函数，这种方法非常依赖于具体的问题背景，例如装箱/背包问题与旅行商邮路问题都是np-hard，但是面对的背景不一样。而且启发函数的好坏对结果影响很大，因而难以泛华。在大数据时代，可以考虑利">
<meta name="twitter:image" content="http://yoursite.com/2019/01/20/2019-1-20-LearningToSolveRoutingProblem/img/MHA-FF.png">
  
    <link rel="alternate" href="/atom.xml" title="Maxwei | My personal blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Maxwei | My personal blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Succes is not final, failure is not fatal. It is the courage to continiue that counts.</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-2019-1-20-LearningToSolveRoutingProblem" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/01/20/2019-1-20-LearningToSolveRoutingProblem/" class="article-date">
  <time datetime="2019-01-19T16:00:00.000Z" itemprop="datePublished">2019-01-20</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/paper/">paper</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      paper reading solve combination problem with NN and RL
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="强化学习解决TSP路径规划问题"><a href="#强化学习解决TSP路径规划问题" class="headerlink" title="强化学习解决TSP路径规划问题"></a>强化学习解决TSP路径规划问题</h1><h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>接触组合优化问题源于参加过的比赛的一些经历，例如华为软挑的云服务器资源分配问题，阿里巴巴调度大赛的虚拟机放置，都属于组合优化问题。传统解决组合优化问题的方法关键在于设计启发式函数，这种方法非常依赖于具体的问题背景，例如装箱/背包问题与旅行商邮路问题都是np-hard，但是面对的背景不一样。而且启发函数的好坏对结果影响很大，因而难以泛华。在大数据时代，可以考虑利用纯数据驱动的方法，利用深度学习、强化学习的理论来学习潜在的启发式方法，进而可以解决这一类问题而不是某一个问题。</p>
<a id="more"></a>
<p>##　算法模型</p>
<p>我目前先针对比较容易理解的tsp问题来探索，TSP的定义很简单，一个二维平面上存在任意多个点，从给定的点出发，遍历所有点，最后回到起点，总的欧式距离最短，假设任意点间的权重仅与距离相关。回顾一下NLP中用来做机器翻译的Seq2Seq模型，输入是一个序列，输出是另外一个序列。这个思想可以很好的用在解决tsp上面，输入是任意的一组点集的排列，输出是得到的距离最短的排列，同样是序列到序列，而且两个序列是等长的。</p>
<p>16年以来，业界利用强化学习＋深度学习的研究热点集中于与机器翻译类似的encode-decode框架，这种想法的开创者来自Google　DeepMind的<a href="https://arxiv.org/pdf/1611.09940.pdf" target="_blank" rel="noopener">论文</a>。到了2017年，Google又提出了一种　<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention is all you need</a>的结构，直接摈弃原来的双RNN模式，全部用attention取代，在机器翻译的效果甚至比原先的encode-decode还要好。</p>
<p>因此，算法的主题框架还是借鉴了actor-critic模式。actor部分由注意力模型(attention model)执行——根据输入，选择一种排列方式并计算代价；critic是一种self-critic，由贪心选择得到的baseline函数来代表。训练方法是经典的蒙特卡洛采样－REINFORCEA算法。下面分别讲述各个部分。</p>
<h2 id="Attention-PG-model"><a href="#Attention-PG-model" class="headerlink" title="Attention-PG model"></a>Attention-PG model</h2><p>定义一个包含n个顶点的实例集合s，每个点的特征由$x_i$表示，包含改点在二维平面中的坐标值。平面中的n个点构成了一个图，这个图是全连接的，包含自连接部分。定义一种遍历方法$\pi＝(\pi_1,…,\pi_n)$是所有点的一种全排列，对于任意$\pi_t\in{\pi_1,…,\pi_n}$以及$t\neq t’$,有$\pi_t \neq \pi_{t’}$. TSP的目标位对于给定的点集s给出一个遍历顺序，使总的距离最少。定义一个随机策略$p(\pi,s)$来根据给定的点集s选择遍历方法。将随机策略用参数$\theta$参数化再分解得到：</p>
<p>$$p_{\theta}=\prod_{t=1}^np_{\theta}(\pi_t,(s,\pi_{1:t-1}))$$</p>
<p>编码器encoder生成所有输入点的embeding值。解码器decoder顺序的生成一个遍历序列$\pi$，每次输出一个点。decoder的输入为encoder的embeding值。对于TSP问题而言，当已经构建了部分遍历路径时，前面已经生成的点是不能再改变的，剩下的问题就是从已生成的最后一个点开始，在剩下还没有遍历的点中找到余下的路径然后回到起点。和已经遍历的点的顺序和坐标没有关系，只要知道了当前的最后一个点和第一个点就可以。decoder将第一个和最后一个点的embeding作为contexts参与下一个节点的解码过程，同时也会接受每个节点的mask标志，表示这个节点是否已经访问过。</p>
<h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head-Attention"></a>Multi-Head-Attention</h3><p>如图１所示是embeding部分，和传统的encoder－decoder结构不一样，由于输入序列的顺序没有实际意义(在机器翻译中，每个单词的语序是有意义的)，因此并没采用LSTM网络处理输入序列，而是采用直接做embeding的方法。如图所示，首先对每个节点$x_i$（维度dx=2）做维度为$d_h=128$的初始化embeding，通过简单的线性投影：</p>
<p>$$h_i^{(0)}=W^xX_i+b^x$$(1)</p>
<p>接下来，初始化后的embeding经过BN后会被送入包含N层的MHA-FF(Multi-Head-Attention)中。</p>
<center><br><img src="img/MHA-FF.png" width="300" height="400" alt="Embeding"></center>

<center>Fig.１　MHA-FF * N</center>

<p>如图１所示，一个MHA-FF层包含两个部分，multi-head-attention 和　feed-forward。在MHA中包含以下操作：</p>
<p>$$\hat{h}_i=BN(h_i^{(l-1)}+MHA_i^l(h_1^{(l-1)},…,h_n^{(l-1)})$$(3)</p>
<p>l-1代表第l-1个MHA层，１..n代表把上一层输入的值分成n个抽头，单独做embeding再连在一起。</p>
<center><br><img src="img/MHA.png" width="500" height="300" alt="Embeding"></center>

<center>Fig.２</center>

<p>首先计算上一层输出的embeding${h_1^{(l-1)},…,h_n^{(l-1)}}$的MHA的值，然后再和原${h_1^{(l-1)},…,h_n^{(l-1)}}$相加。</p>
<p>在全连接层中再做一次线性变换：</p>
<p>$$h_i^{(l)}=BN(\hat{h}_i+FF(\hat{h}_i)$$(4)</p>
<p>FF层中包含一个隐层，神经元数量为512采用ReLu函数激活。BN表示batch normaliztion</p>
<p>对应的代码如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Multi Attention 部分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multihead_attention</span><span class="params">(inputs, num_units=None, num_heads=<span class="number">16</span>, dropout_rate=<span class="number">0.1</span>, is_training=True)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"multihead_attention"</span>, reuse=<span class="literal">None</span>):</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># projection every point to attain Q,K,V, respectively</span></span><br><span class="line">        Q = tf.layers.dense(inputs, num_units, activation=tf.nn.relu) <span class="comment"># [batch_size, seq_length, n_hidden]</span></span><br><span class="line">        K = tf.layers.dense(inputs, num_units, activation=tf.nn.relu) <span class="comment"># [batch_size, seq_length, n_hidden]</span></span><br><span class="line">        V = tf.layers.dense(inputs, num_units, activation=tf.nn.relu) <span class="comment"># [batch_size, seq_length, n_hidden]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Split and concat</span></span><br><span class="line">        Q_ = tf.concat(tf.split(Q, num_heads, axis=<span class="number">2</span>), axis=<span class="number">0</span>) <span class="comment"># [batch_size, seq_length, n_hidden/num_heads]</span></span><br><span class="line">        K_ = tf.concat(tf.split(K, num_heads, axis=<span class="number">2</span>), axis=<span class="number">0</span>) <span class="comment"># [batch_size, seq_length, n_hidden/num_heads]</span></span><br><span class="line">        V_ = tf.concat(tf.split(V, num_heads, axis=<span class="number">2</span>), axis=<span class="number">0</span>) <span class="comment"># [batch_size, seq_length, n_hidden/num_heads]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Multiplication</span></span><br><span class="line">        outputs = tf.matmul(Q_, tf.transpose(K_, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>])) <span class="comment"># num_heads*[batch_size, seq_length, seq_length]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Scale</span></span><br><span class="line">        outputs = outputs / (K_.get_shape().as_list()[<span class="number">-1</span>] ** <span class="number">0.5</span>)</span><br><span class="line">  </span><br><span class="line">        <span class="comment"># Activation</span></span><br><span class="line">        outputs = tf.nn.softmax(outputs) <span class="comment"># num_heads*[batch_size, seq_length, seq_length]</span></span><br><span class="line">          </span><br><span class="line">        <span class="comment"># Dropouts</span></span><br><span class="line">        outputs = tf.layers.dropout(outputs, rate=dropout_rate, training=tf.convert_to_tensor(is_training))</span><br><span class="line">               </span><br><span class="line">        <span class="comment"># Weighted sum</span></span><br><span class="line">        outputs = tf.matmul(outputs, V_) <span class="comment"># num_heads*[batch_size, seq_length, n_hidden/num_heads]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Restore shape</span></span><br><span class="line">        outputs = tf.concat(tf.split(outputs, num_heads, axis=<span class="number">0</span>), axis=<span class="number">2</span> ) <span class="comment"># [batch_size, seq_length, n_hidden]</span></span><br><span class="line">              </span><br><span class="line">        <span class="comment"># Residual connection</span></span><br><span class="line">        outputs += inputs <span class="comment"># [batch_size, seq_length, n_hidden]</span></span><br><span class="line">              </span><br><span class="line">        <span class="comment"># Normalize</span></span><br><span class="line">        outputs = tf.layers.batch_normalization(outputs, axis=<span class="number">2</span>, training=is_training, name=<span class="string">'ln'</span>, reuse=<span class="literal">None</span>)  <span class="comment"># [batch_size, seq_length, n_hidden]</span></span><br><span class="line"> </span><br><span class="line">    <span class="keyword">return</span> outputs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前馈连接部分</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">feedforward</span><span class="params">(inputs, num_units=[<span class="number">2048</span>, <span class="number">512</span>], is_training=True)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"ffn"</span>, reuse=<span class="literal">None</span>):</span><br><span class="line">        <span class="comment"># Inner layer</span></span><br><span class="line">        params = &#123;<span class="string">"inputs"</span>: inputs, <span class="string">"filters"</span>: num_units[<span class="number">0</span>], <span class="string">"kernel_size"</span>: <span class="number">1</span>, <span class="string">"activation"</span>: tf.nn.relu, <span class="string">"use_bias"</span>: <span class="literal">True</span>&#125;</span><br><span class="line">        outputs = tf.layers.conv1d(**params)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># out layer</span></span><br><span class="line">        params = &#123;<span class="string">"inputs"</span>: outputs, <span class="string">"filters"</span>: num_units[<span class="number">1</span>], <span class="string">"kernel_size"</span>: <span class="number">1</span>, <span class="string">"activation"</span>: <span class="literal">None</span>, <span class="string">"use_bias"</span>: <span class="literal">True</span>&#125;</span><br><span class="line">        outputs = tf.layers.conv1d(**params)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Residual connection</span></span><br><span class="line">        outputs += inputs</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Normalize</span></span><br><span class="line">        outputs = tf.layers.batch_normalization(outputs, axis=<span class="number">2</span>, training=is_training, name=<span class="string">'ln'</span>, reuse=<span class="literal">None</span>)  <span class="comment"># [batch_size, seq_length, n_hidden]</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure></p>
<p>定义$h_i^{(l)}$是第$l$层的MHA-FF输出，encoder除了计算每个节点的embeding之外还会额外计算一个扩展的embeding值作为补充，这个值定义为$\hat{h}^{(N)}$:</p>
<p>$$\hat{h}^{(N)} = \frac{1}{N}\sum_{i=1}^Nh_i^{(n)}$$(2)</p>
<p>所有的节点的embeding值$h_i^{(N)}$和扩展embeding值$\hat{h}^{(N)}$将共同作为decoder的输入。</p>
<p>MHA 和 FF 部分会重复n次，最后</p>
<h3 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h3><center><br><img src="img/decode.jpg" width="600" height="400" alt="Embeding"></center>

<p>如图所示为decoder的解码过程。decoder的输入一共包含两个部分：所有节点embeding值${h_1^{(N)},…,h_n^{(N)}}$以及context vector(左上角黄色小框中的三个向量)。其中context vector包含３个部分，Graph embeding的值$\hat{h}^{(N)}$,也就是扩展的embeding值，另外两个是已生成序列中的第一个和最后一个节点的embeding值。初始化时为$d_h$维的向量$V^f$和$V^l$。</p>
<p>利用attention　mechanism来解码的过程如下：</p>
<p>首先利用context vector计算问询值$q_c$:</p>
<p>$$q_c = W^Qh_{(c)}$$</p>
<p>再利用所有点集的embeding值计算attention中的key和value：</p>
<p>$$ki = W^Kh_i$$</p>
<p>$$vi = W^Vh_i$$</p>
<p>得到q、k、v之后计算attention的过程如下：</p>
<p>$$<br>\mu_{(c)j} =<br>\begin{cases}<br>  C\tanh(\frac{q_c^Tk_j}{\sqrt{d_k}}), &amp; j \neq \pi_{t’}, t’ &lt; t\<br>  -\infty, &amp; \text{otherwise}<br>\end{cases}<br>$$</p>
<p>上式计算的是询问值$q_c$和每个没有访问过的节点的相似度$u_{(c)j}$,对于已经访问过的节点，相似度设置为负无穷。利用tanh函数将计算结果约束在[-C, C]之间，接下来计算每个节点的输出概率：</p>
<p>$$p_i = p_{\theta}(\pi_t=i)=\frac{e^{\mu_{cj}}}{\sum_je^{\mu_{cj}}}$$</p>
<p>将输出的新节点对应的embeding值作为下一次解码的contex vector中的最后一个节点的embeding值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention</span><span class="params">(self,ref,query)</span>:</span></span><br><span class="line">　　　　　“”“</span><br><span class="line">    ref: encode output</span><br><span class="line">    query: context hidden unit h_N</span><br><span class="line">　　　　　”“”</span><br><span class="line">    <span class="comment"># Attending mechanism</span></span><br><span class="line">    encoded_ref_g = tf.nn.conv1d(ref, self.W_ref_g, <span class="number">1</span>, <span class="string">"VALID"</span>, name=<span class="string">"encoded_ref_g"</span>) <span class="comment"># [Batch size, seq_length, n_hidden]</span></span><br><span class="line">    encoded_query_g = tf.expand_dims(tf.matmul(query, self.W_q_g, name=<span class="string">"encoded_query_g"</span>), <span class="number">1</span>) <span class="comment"># [Batch size, 1, n_hidden]</span></span><br><span class="line">    scores_g = tf.reduce_sum(self.v_g * tf.tanh(encoded_ref_g + encoded_query_g), [<span class="number">-1</span>], name=<span class="string">"scores_g"</span>) <span class="comment"># [Batch size, seq_length]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># # Attend to current city and cities to visit only</span></span><br><span class="line">    attention_g = tf.nn.softmax(scores_g - <span class="number">100000000.</span>*(self.mask - self.first_city_hot),name=<span class="string">"attention_g"</span>)  </span><br><span class="line">    self.attending.append(attention_g)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1 glimpse = Linear combination of reference vectors (defines new query vector)</span></span><br><span class="line">    glimpse = tf.multiply(ref, tf.expand_dims(attention_g,<span class="number">2</span>))</span><br><span class="line">    glimpse = tf.reduce_sum(glimpse,<span class="number">1</span>)+query <span class="comment"># 残差连接</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Pointing mechanism with 1 glimpse</span></span><br><span class="line">    encoded_ref = tf.nn.conv1d(ref, self.W_ref, <span class="number">1</span>, <span class="string">"VALID"</span>, name=<span class="string">"encoded_ref"</span>) <span class="comment"># [Batch size, seq_length, n_hidden]</span></span><br><span class="line">    encoded_query = tf.expand_dims(tf.matmul(glimpse, self.W_q, name=<span class="string">"encoded_query"</span>), <span class="number">1</span>) <span class="comment"># [Batch size, 1, n_hidden]</span></span><br><span class="line">    scores = tf.reduce_sum(self.v * tf.tanh(encoded_ref + encoded_query), [<span class="number">-1</span>], name=<span class="string">"scores"</span>) <span class="comment"># [Batch size, seq_length]</span></span><br><span class="line">    <span class="keyword">if</span> self.inference_mode == <span class="literal">True</span>:</span><br><span class="line">        scores = scores/self.temperature <span class="comment"># control diversity of sampling (inference mode)</span></span><br><span class="line">    scores = self.C*tf.tanh(scores) <span class="comment"># 约束分值</span></span><br><span class="line"></span><br><span class="line">    masked_scores = scores - <span class="number">100000000.</span>*self.mask <span class="comment"># 访问过的节点设为负穷大，输出概率为０．</span></span><br><span class="line">    pointing = tf.nn.softmax(masked_scores, name=<span class="string">"attention"</span>) <span class="comment"># 归一化概率值</span></span><br><span class="line">    self.pointing.append(pointing)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> masked_scores</span><br></pre></td></tr></table></figure>
<h3 id="REINFORCE-with-greedy-baseline"><a href="#REINFORCE-with-greedy-baseline" class="headerlink" title="REINFORCE　with greedy baseline"></a>REINFORCE　with greedy baseline</h3><p>定义一个概率分布$p_{\theta}(\pi,s)$,意为给定实例点集s，输出遍历序列$\pi$的概率。定义损失函数为生成遍历序列总长度的期望。损失函数梯度计算如下：</p>
<p>$$L(\theta,s)=E_{p_{\theta}(\pi,s)}[(L(\pi)-b(s))\nabla\log p_{\theta}(\pi,s)]$$</p>
<p>sutton说一个好的baseline函数可以降低梯度方差从而加速学习过程。将b(s)的定义为目前为止最好的模型输出的解对应的代价。从直观上来理解，我们的目的是向梯度最斗的方向下降，以及代价函数越小越好，当当前的估计值$L(\pi)$比之前最好模型得到的$b(s)$还要好时，$L(\pi)-b(s)$就是负值，会加速下降。这种思想类似于self-critic的方法，自我更新，没有训练第二个critic网络。</p>
<p><center><br><img src="img/REINFORCE.png"></center></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2019/01/20/2019-1-20-LearningToSolveRoutingProblem/" data-id="cjv8u8wzz000k59bt8piktu2v" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/03/08/2019-3-8-shell/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          shell learning basic
        
      </div>
    </a>
  
  
    <a href="/2019/01/15/2019-1-15-paper-reading-ncp/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">paper reading solve combination problem with NN and RL</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/DSA/">DSA</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/LeetCode/">LeetCode</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Meachine-Learning/">Meachine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tensorflow/">Tensorflow</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/c/">c++</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/paper/">paper</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/reinforce-learning/">reinforce learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/shell/">shell</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/比赛/">比赛</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/编译原理/">编译原理</a></li></ul>
    </div>
  </div>


  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">May 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/03/">March 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">November 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/10/">October 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/10/">October 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2019/05/03/leetcode/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/05/03/code/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/05/03/2019-orderMatters/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/05/03/2019-3-28－强化学习概览/">(no title)</a>
          </li>
        
          <li>
            <a href="/2019/05/03/2019-1-14-keras-learningScript/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Fang Wei<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>